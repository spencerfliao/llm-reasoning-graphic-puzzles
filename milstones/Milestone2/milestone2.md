# COLX 523 - Advanced Corpus Linguistics

# Milestone 2                   

## Corpus + explanation
### Overview
Our corpus consists of two components. The first is the ARC dataset originally introduced by Fran√ßois Chollet and published on Kaggle. The dataset consists of 400 training examples, 400 evaluation examples and 100 testing examples. Each example is a distinct reasoning problem where the task is to go from a given input to an output. Each example consists of three training pairs of inputs and outputs that illustrate the logic or pattern to be identified and applied to solve the test pair. This portion of the corpus was already present on Kaggle, and we just had to convert it into a more human-readable format for the next part, i.e. we had to convert the original JSON files into images. 

The next component is the human-annotated explanations for each ARC reasoning problem. Our team is working through each problem, creating detailed explanations that break down the logic or pattern identified in the training pairs and how it applies to the test pair. These annotations aim to articulate the step-by-step thought process a human might use to solve these complex abstract reasoning. This component is one of the main contributions of our project. By the end of week three, we hope to have annotated about 200 examples and then use an LLM like GPT4 to annotate the remaining 600 examples. From there, once we have reviewed the annotations done by the LLM, we plan to fine-tune an LLM like LLaVA on the ARC dataset and the corresponding annotations to see if it can successfully solve the reasoning problems of the test set.

### Sources
1. ARC Challenge Dataset Source: [Kaggle ARC Challenge Dataset](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview) (Original source of ARC challenge problems)
2. Visual representations of the ARC Dataset: [GitHub Repository](https://github.ubc.ca/MDS-CL-2023-24/COLX_523_ARCOT/tree/master/data/images) (Images generated from the ARC dataset to make the task of annotations easier)

### Corpus Format
The original ARC dataset is stored in JSON format, designed for compatibility with various machine-learning tools. It is divided into three folders: training, evaluation and test. Each contains multiple JSON files representing distinct reasoning problems, including inputs and outputs designed to test abstract reasoning abilities. The training and evaluation examples also have images corresponding to each JSON file. Each image is a visual representation of a reasoning problem. The human annotations are still a work in progress, so the final storage format and database structure will be determined based on the scalability needs and the ease of access for both machine learning models and researchers. For now we are storing them in TSV files.

## Corpus collection code
### Overview
The code is used to convert JSON files into PNG files, which, along with the human annotations, form a part of the corpus. It processes the training and evaluation data, generates images based on this data, combines these images with specific configurations, and saves the resulting images along with metadata about the data. The format conversion is necessary to provide a visual aid for manually writing the annotations. The code can be found in a Python file named utils.py, located inside the src directory at the root level.

### Libraries
The code utilizes several libraries such as json, os, shutil, numpy, pandas, PIL (Python Imaging Library), and matplotlib for file handling, data manipulation, image processing, and image generation, respectively.

### Functions
1. `load_json(file_path)`: Loads and returns data from a JSON file. It takes a file path as an argument and returns the parsed JSON data.
2. `add_margin(pil_img, top, right, bottom, left, color)`: Adds a margin around a PIL image and returns the new image with the margin. It takes various aspects of the margin as arguments.
3. `delete_directory(path)`: Deletes a directory at the given path. It handles exceptions to avoid crashes if the directory does not exist or other errors occur during deletion.
4. `generate_image(array, file_path, title)`: Generates and saves an image based on a 2D array where each element represents a color coded by integers. It maps these integers to specific colors, creates a plotted image with these colors, and saves it to a file path with a given title.
5. `generate_and_combine_images(data, output_dir, filename, temp_dir='./temp')`: Processes input data to generate images for training and test datasets, adds margins, combines them horizontally and vertically, and saves the final combined image. It cleans up by deleting temporary images and directories used during processing.
6. `get_metadata(input_data)`: Extracts and returns metadata from the input data, such as the number of rows and columns in the input and output data.

### Algorithm
1. Set up directories for output, training, and evaluation images.
2. Iterate over JSON files in the training and evaluation directories.
3. For each JSON file:
   1. Load the data.
   2. Generate and combine images based on the data.
   3. Extract metadata from the data.
4. Save the metadata for training and evaluation datasets as CSV files in the specified output directory.

## Corpus analysis (Optional)
rubric={reasoning:2}

You can get bonus points in this milestone by doing an in-depth analysis of your corpus, using the kinds of approaches we talked about in COLX 521. Compare the statistical properties of your corpus to other corpora with interesting similarities and/or differences. If you have metadata, provide a discussion of any interesting interaction between the metadata and the corpus statistics you derive. You should do this in a separate .md file (corpus_analysis.md), and include links to any code you write to carry out this analysis.           

## Annotation plan
### Description of Annotations
We provide a high-level overview of our annotation process, focusing on human natural language descriptions for each ARC task. Specifically, we generate six sentence-level annotations for each task: Reflection, Pixel Changes, Object Changes, Helper Functions, Overall Pattern, and Program Instructions. Here's a breakdown of each section:

- Reflection: Offers a concise abstract overview of the task.
- Pixel Changes: Describes pixel-level relationships between input and output pairs, including movements, color alterations, or pattern variations.
- Object Changes: Defines objects as connected pixel sets, often sharing the same color. This section outlines object-level relationships, covering movements, shapes, counts, sizes, positions, or colors.
- Helper Functions: Lists Python helper functions applicable to each task and suggests their potential use.
- Overall Pattern: Summarizes pixel and object changes, providing a simpler description of the input-output relationship.
- Program Instructions: Offers a plan or pseudocode for writing a Python function to solve the ARC task.

Informed by Kumar et al.'s findings on neural models learning human inductive bias through natural language abstraction [1], we prioritize abstract descriptions. For instance, if a pattern resembles an axe, we describe it as such rather than focusing on pixel, row, or column specifics. This approach aims to enhance accuracy in solving ARC tasks, leveraging the handcrafted nature of the dataset. However, this method may not be effective for automatically generated ARC tasks.

### Tools for Annotation
We've developed a script to convert all ARC tasks from JSON to images, which are then equally distributed among annotators. Annotation occurs on a shared Google Sheet, facilitating collaboration. Upon completion, the annotations will be exported to a TSV file.

### Annotators
Project team members are responsible for annotations, with each member handling a quarter of the total workload. We opt not to utilize external resources like Mechanical Turk due to the technical nature of the task, expecting higher quality and alignment with our goals from team members' involvement.

### Expectations
We anticipate annotating approximately 200 ARC tasks in total, with each annotator responsible for 50 annotations. Estimated at 10 minutes per annotation, each annotator will spend approximately 8 hours in total. This estimate is based on a pilot study involving four annotations. Combined with paired data from GPT4 in future work, we believe this dataset will be adequate for bootstrapping a Reinforcement Learning from Human Feedback (RLHF) training on a Large Language Model (LLM).

### Data Quality
Pilot studies have been conducted on a few annotations, guiding our annotation approach. Annotators will cross-check each other's work to maintain high quality. Additionally, annotator training will cover technical details, such as the meaning of helper functions, and ensure consistency in writing style.

### Pilot Study Report
We conducted two phases of pilot studies, annotating 14 samples in total. Initially, we focused solely on reflections, but subsequent discussions highlighted issues like data alignment and color representation. In the second trial, we annotated all specified sections, streamlined the process with Google Sheets and image conversion scripts, and refined our annotation style to better suit our goals.

## Annotation materials
rubric={reasoning:2,writing:1,raw:1}

Annotation guidelines for a task are provided in the form of various sections of instructions, and we will be annotating the sections ourselves.
Some examples of data processed into visually appropriate format and their annotations are shown below: <br>
Example 1: <br>
<img src="https://media.github.ubc.ca/user/2993/files/ee0343e1-a2ce-43d5-b343-3057bd1a5637" width="500" height="auto"> <br>
Example 2: <br>
<img src="https://media.github.ubc.ca/user/2993/files/b94f58da-17d9-44b0-8000-254765cc7587" width="500" height="auto"> <br>
| Example No. | Reflections                                                                                                                                                                                                                                                                                        | Pixel Changes                                                                                                                                                                                                             | Object Changes                                                                                                                                                                                                               | Helper Functions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Overall Pattern                                                                                                                         | Program Instructions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1           | In this task, there are two distinct colored cells present in the input gride. These two colored cells are present at opposite ends of the grid. In the output gride these two colored cells transform into horizontal/vertical lines of cells and form an alternative repeating pattern of lines. | Each of the two colored cells transform into multiple horizontal (or vertical) lines of colored cells. Wether the two colored cells form horizontal lines or vertical depends on their initial position in the input grid | The two colored cells are converted into mulitple colored lines. The colored lines from an alternating pattern and are parallel to each other. They are seperated by one or more black lines that the background in made of. | The main pieces of information are needed to solve this problem is the position of the colored cells with respect to the input grid. We basically want to know if the two cells are at the upper and lower ends of the grid or the left and right ends. For this we can use the get_loc function. get_seperation can be used to find the horizontal/ vertical seperation between the colored  cells. Which out of the two should the func return will depend on the output of get_loc. Finally create_pattern can be used to fill the cells of the output grid to create the alternating pattern. | The two colored cells are streatched to from two colored lines and then this pattern is repeated as much as possible in the output grid | 1. Use get_loc to figure out if the two colored cells are at the top and bottom ends or left and right ends. 2. Use get_seperation to find the min of the horizontal seperation and vertical seperation. If the two cells are at top and bottom, the function should return the horizontal seperation and if they are at left and right, then it should return the vertical seperation. 3. Use the create_pattern to fill cells vertically/horizontally starting from the colored cells in a straight line. Then duplicate the pattern while maintaining the seperation calculated above between the lines. |
| 2           | In this task, there are four partially filled rectangles, one at each corner of the input gride. Three out of the four have the same color. The output is the partially filled rectangle who's color does not match the other three.                                                               | The output grid is in the same configuration as one of the colored pixel grids.  Out of the two colors present in the input, the output is the colored pixel grid of the odd one out color.                               | Out of the four colored objects, the output in the colored object whoes color does not matchs with the color of the other three colored objects.                                                                             | The main piece of information needed is the position of each of the four partially colored rectangles. get_objects function can be used to get this information. get_color can be used to get the color of each object and generate_object can be used to generate the duplicate of the object whos color does not match with the other three in the output grid.                                                                                                                                                                                                                                 | The main goal here to pick the colored object whos color does not match the other three objects.                                        | 1. Use the get_objects to get the four colored objects from the input gride. 2. Use get_color on each of the for objects and pick the one that does not match with the other three. 3. Use the generate_object to create a copy of the picked object in the output gird.                                                                                                                                                                                                                                                                                                                                    |

## References
1. Kumar, Sreejan, et al. "Using natural language and program abstractions to instill human inductive biases in machines." Advances in Neural Information Processing Systems 35 (2022): 167-180.
